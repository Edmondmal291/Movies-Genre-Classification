{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda4dd5a-2f9c-46fc-92b5-65e87614c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tkinter import filedialog as fd\n",
    "import pysrt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281b2fea-1b22-4861-a93b-79836bb95b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/emalone/Documents/CSS6625/training_data/Blade_1.srt\n",
      "/home/emalone/Documents/CSS6625/training_data/Demolition_Man.srt\n",
      "/home/emalone/Documents/CSS6625/training_data/Drag_Me_To_Hell.srt\n",
      "/home/emalone/Documents/CSS6625/training_data/Halloween_II.srt\n",
      "/home/emalone/Documents/CSS6625/training_data/IndependenceDay.brrip.1996.1080p.srt\n",
      "['Blade_1', 'Demolition_Man', 'Drag_Me_To_Hell', 'Halloween_II', 'IndependenceDay.brrip.1996.1080p']\n"
     ]
    }
   ],
   "source": [
    "# extract the movie name from the file path\n",
    "movie_list = []\n",
    "srt_file_paths = fd.askopenfilenames(title='Open files',initialdir='~/Documents/CSS6625/training_data/')\n",
    "for file_path in srt_file_paths:\n",
    "    print(file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    movie_name = os.path.splitext(file_name)[0]\n",
    "    movie_list.append(movie_name)\n",
    "    #print(movie_name)\n",
    "print(movie_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc772fc-a5da-4334-8e40-822bc6f07b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [movie_name, god, help, kill, please, dead, love, sorry, beautiful, baby, father, bomb]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "word_bank = ['god','help','kill','please','dead','love','sorry','beautiful','baby','father','bomb']\n",
    "movie_data = []\n",
    "movie_row = []\n",
    "common_words = []\n",
    "movie_counter = 0\n",
    "cell_counter = 0\n",
    "\n",
    "\n",
    "if(len(srt_file_paths) > 0):\n",
    "    # Iterate over the tuple of file paths\n",
    "    for file_path in srt_file_paths:\n",
    "        #print(file_path)\n",
    "        srt_file = pysrt.open(file_path)\n",
    "        subs = srt_file\n",
    "        # extract only the html tags and content from the raw srt file and print to screen.\n",
    "        html_text_list = []\n",
    "        html_tags_with_text = \"\"\n",
    "        counter = 0\n",
    "        srt_total_line_numebers = len(subs)\n",
    "        while counter < srt_total_line_numebers:\n",
    "            html_tags_with_text += subs[counter].text\n",
    "            html_text_list.append(subs[counter].text)\n",
    "            counter +=1\n",
    "        # Put a space between each element to create a string \n",
    "        corpus_with_tags = ''.join(html_text_list)\n",
    "        #print(corpus_with_tags)\n",
    "        # Parse the text from in between the html tags in to memoryy\n",
    "        soup = BeautifulSoup(corpus_with_tags, 'html.parser')\n",
    "        corpus = soup.get_text().lower()\n",
    "        #print(corpus)\n",
    "        # Tokenizing the sentence into indvidual words.\n",
    "        tokenize_words =  word_tokenize(corpus)\n",
    "        #print(tokenize_words)\n",
    "        \n",
    "        #frequency_distribution = FreqDist(tokenize_words)\n",
    "        # Filter out the stop words and place the filtered words into a seperate list.  \n",
    "        filtered_list = []\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        for word in tokenize_words:\n",
    "            if word.casefold() not in stop_words:\n",
    "                filtered_list.append(word)\n",
    "                if (word == ' kill' or word ==\" Kill\"):\n",
    "                    print(\"found word {0}\".format(word))\n",
    "        #print(filtered_list)\n",
    "        # Display the most frequent word that is contain in the list.\n",
    "        frequency_distribution = FreqDist(filtered_list)\n",
    "        # add movie name in the first element\n",
    "        movie_row.append(movie_list[movie_counter])\n",
    "        common_words = frequency_distribution.most_common(300)\n",
    "        #len(common_words)\n",
    "        print(len(common_words))\n",
    "        missing_common_word_counter = 0\n",
    "        for word in word_bank:\n",
    "            #print(\"the common word is {0}\".format(common_words))\n",
    "            #for word in word_bank:\n",
    "            for common_word in common_words:\n",
    "                #print(\"the word bank is {0} and the common word is {1}\".format(word,common_words))\n",
    "                if (word == common_word[0]):\n",
    "                    #print(\"found match#{0}\".format(word)) \n",
    "                    cell_counter += 1\n",
    "                    movie_row.append(common_word[1])\n",
    "                    missing_common_word_counter = 0\n",
    "                else:\n",
    "                    missing_common_word_counter += 1\n",
    "                    if(missing_common_word_counter == len(common_words)):\n",
    "                        movie_row.append(0)\n",
    "            missing_common_word_counter = 0\n",
    "\n",
    "        print(movie_row)\n",
    "        movie_data.append(movie_row)\n",
    "        movie_row = []\n",
    "        print(movie_counter)\n",
    "        movie_counter += 1\n",
    "        cell_counter = 0\n",
    "        # iterate over the words to create training data \n",
    "elif(len(srt_file_paths) == 0):\n",
    "   print(\"select a file\")\n",
    "movie_counter = 0\n",
    "print(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149c50c-4f8c-4299-a183-65ee0a581635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe with movie_data from previous steps\n",
    "df = pd.DataFrame(movie_data, columns= ['movie_name','god','help','kill','please','dead','love','sorry','beautiful','baby','father','bomb'] )\n",
    "df[\"genre\"]= \"\"\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea097865-cefb-4b00-a193-f57699359b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display graphs of explatory data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3df9dc-4e1f-434d-a097-bc325325c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to csv to for labeleing the data\n",
    "df.to_csv('/home/emalone/Documents/CSS6625/training_data/movie_training_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16bbc1-9d17-49e1-9d96-7628fd3b0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through data and label the data Bold text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420da3eb-c4ef-465c-873a-bb0b07e355dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "train_data = pd.read_csv('/home/emalone/Documents/CSS6625/training_data/movie_training_data.csv')\n",
    "print(train_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4810d7-2bf4-4586-8785-90dd1f4493b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [movie_name, god, help, kill, please, dead, love, sorry, beautiful, baby, father, bomb]\n",
      "Index: []\n",
      "['Blade_1', ('dead', 5), ('kill', 5), ('please', 4), ('help', 4), ('god', 4), ('baby', 3), ('sorry', 3)]\n",
      "['Demolition_Man', ('kill', 11), ('sorry', 8), ('love', 7), ('please', 6), ('god', 3)]\n",
      "['Drag_Me_To_Hell', ('god', 22), ('help', 16), ('dead', 14), ('love', 10), ('please', 10), ('baby', 8), ('sorry', 7), ('father', 2)]\n",
      "['Halloween_II', ('please', 53), ('help', 22), ('god', 19), ('baby', 13), ('love', 12), ('sorry', 9), ('dead', 8), ('kill', 8), ('beautiful', 2)]\n"
     ]
    }
   ],
   "source": [
    "# create pandas dataframe\n",
    "#column_names =['movie_name','god','help','kill','please','dead','love','sorry','beautiful','baby','father','bomb']\n",
    "df = pd.DataFrame(columns= ['movie_name','god','help','kill','please','dead','love','sorry','beautiful','baby','father','bomb'] )\n",
    "print(df)\n",
    "for (row in movie_data):\n",
    "    print(row)\n",
    "    for (move_word in row):\n",
    "        \n",
    "#df = df.columns = ['movie_name','god','help','kill','please','dead','love','sorry','beautiful','baby','father','bomb'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e29447-98ea-4972-9d20-207e631d42dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in SRT file\n",
    "# open file explorer to select one or multiple srt files.\n",
    "srt_file_paths = fd.askopenfilenames(title='Open files',initialdir='~')\n",
    "\n",
    "if (len(srt_file_paths) == 0):\n",
    "   print(\"select a file\") \n",
    "elif (len(srt_file_paths) == 1):\n",
    "    subs = pysrt.open(srt_file_paths[0])\n",
    "else:\n",
    "    # Iterate over the tuple of file paths\n",
    "    for file_path in srt_file_paths:\n",
    "        srt_file = pysrt.open(file_path)\n",
    "        # concatanate string togethor \n",
    "        subs +=srt_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785cf8e-1472-4918-af38-0f0b6504e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the html tags and content from the raw srt file and print to screen.\n",
    "html_text_list = []\n",
    "html_tags_with_text = \"\"\n",
    "counter = 0\n",
    "srt_total_line_numebers = len(subs)\n",
    "\n",
    "while counter < srt_total_line_numebers:\n",
    "    html_tags_with_text += subs[counter].text\n",
    "    html_text_list.append(subs[counter].text)\n",
    "    counter +=1\n",
    "# put a space between each element to create a string \n",
    "corpus_with_tags = ''.join(html_text_list)\n",
    "print(corpus_with_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f8599-4368-405d-a6d8-2a01f9a3de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text from in between the html tags in to memoryy\n",
    "soup = BeautifulSoup(corpus_with_tags, 'html.parser')\n",
    "corpus = soup.get_text()\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c56001-bf94-4ebe-a6ff-2b3310c6221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the sentence into indvidual words.\n",
    "tokenize_words =  word_tokenize(corpus)\n",
    "print(tokenize_words)\n",
    "frequency_distribution = FreqDist(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a4138-7864-4af9-bc46-e3167ccfdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the stop words and place the filtered words into a seperate list.  \n",
    "filtered_list = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "for word in tokenize_words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)\n",
    "        if (word == ' kill' or word ==\" Kill\"):\n",
    "            print(\"found word {0}\".format(word))\n",
    "#print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75743c04-25a5-478a-8de6-a119df4c7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the most frequent word that is contain in the list.\n",
    "frequency_distribution = FreqDist(filtered_list)\n",
    "frequency_distribution.most_common(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c28d0-983b-417a-b43d-acc14fd3d79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb5a28-2c58-4147-9ff6-c9c4c87c4653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896138b-3384-4eff-87ff-49a9ff184bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
